{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from PIL import Image\n",
    "from nltk.translate import bleu_score\n",
    "\n",
    "# Import Custom Module\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config import IMAGES_DIR\n",
    "from src.utils.sets import (\n",
    "    init_image_descriptions_map,\n",
    "    load_set_images,\n",
    "    init_image_descriptions_map_set,\n",
    "    data_generator\n",
    ")\n",
    "\n",
    "from src.utils.image import load_image, load_image_embedding_map\n",
    "from src.utils.sequence import clean, init_idx_word_map, init_word_idx_map\n",
    "\n",
    "from src.nn import NeuralImageCaptioning, NICInference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding and formating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A brown and white dog is running through the snow .',\n",
       " 'A dog is running in the snow',\n",
       " 'A dog running through snow .',\n",
       " 'a white and brown dog is running through a snow covered field .',\n",
       " 'The white and brown dog is running over the surface of the snow .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image2descriptions = init_image_descriptions_map()\n",
    "\n",
    "image2descriptions['101654506_8eb26cfb60']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a hello are you'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(\n",
    "    'A HellO! How12 are yoU>??'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name, descriptions in image2descriptions.items():\n",
    "    image2descriptions[img_name] = [clean(descr) for descr in descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a brown and white dog is running through the snow',\n",
       " 'a dog is running in the snow',\n",
       " 'a dog running through snow',\n",
       " 'a white and brown dog is running through a snow covered field',\n",
       " 'the white and brown dog is running over the surface of the snow']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image2descriptions['101654506_8eb26cfb60']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for key in image2descriptions.keys():\n",
    "    [vocabulary.update(descr.split()) for descr in image2descriptions[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.update('0')\n",
    "vocabulary.update(['<START>'])\n",
    "vocabulary.update(['<END>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8767\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort the Vocabulary lexically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Idx and Idx2Word maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = init_word_idx_map(vocabulary)\n",
    "idx2word = init_idx_word_map(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2224"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[2224]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Description Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptions: 40460\n"
     ]
    }
   ],
   "source": [
    "descriptions = []\n",
    "\n",
    "for key, val in image2descriptions.items():\n",
    "    for descr in val:\n",
    "        descriptions.append(descr)\n",
    "\n",
    "print('Total number of descriptions: {}'.format(len(descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longes description:\n",
      "\n",
      "\"an africanamerican man wearing a green sweatshirt and blue vest is holding up dollar bills in front of his face while standing on a busy sidewalk in front of a group of men playing instruments\"\n"
     ]
    }
   ],
   "source": [
    "longets_description = max(descriptions, key=lambda x: len(x.split()))\n",
    "\n",
    "print('Longes description:\\n\\n\"{}\"'.format(longets_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Description Length: 37\n"
     ]
    }
   ],
   "source": [
    "max_description_length = len(longets_description.split()) + 2\n",
    "\n",
    "print('Max Description Length: {}'.format(max_description_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Dev/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6000\n",
      "Dev size: 1000\n",
      "Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "train_images = load_set_images('train')\n",
    "dev_images = load_set_images('dev')\n",
    "test_images = load_set_images('test')\n",
    "\n",
    "print('Train size: {}'.format(len(train_images)))\n",
    "print('Dev size: {}'.format(len(dev_images)))\n",
    "print('Test size: {}'.format(len(test_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Image->Descriptions Maps Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6000\n",
      "Dev size: 1000\n",
      "Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "train_image2descriptions = init_image_descriptions_map_set(train_images, image2descriptions)\n",
    "dev_image2descriptions = init_image_descriptions_map_set(dev_images, image2descriptions)\n",
    "test_image2descriptions = init_image_descriptions_map_set(test_images, image2descriptions)\n",
    "\n",
    "print('Train size: {}'.format(len(train_image2descriptions)))\n",
    "print('Dev size: {}'.format(len(dev_image2descriptions)))\n",
    "print('Test size: {}'.format(len(test_image2descriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a black dog is running after a white dog in the snow',\n",
       " 'black dog chasing brown dog through snow',\n",
       " 'two dogs chase each other across the snowy ground',\n",
       " 'two dogs play together in the snow',\n",
       " 'two dogs running through a low lying body of water']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image2descriptions['2513260012_03d33305cf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Image-Embedding Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"train\" Image-Embedding Map loaded.\n"
     ]
    }
   ],
   "source": [
    "train_image2embedding = load_image_embedding_map('train', train_image2descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "NUM_HIDDEN_NEURONS = [256, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "![NIC_model](../../img/NIC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "neural_image_captioning = NeuralImageCaptioning(\n",
    "    EMBEDDING_DIM, max_description_length, len(vocabulary), NUM_HIDDEN_NEURONS, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "image_embedding_input (InputLay (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequence_input (InputLayer)     (None, 37)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "top_image_encoder (Model)       (None, 1, 300)       2405676     image_embedding_input[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sequence_decoder (Model)        (None, 37, 8767)     5978899     sequence_input[0][0]             \n",
      "                                                                 top_image_encoder[1][0]          \n",
      "==================================================================================================\n",
      "Total params: 8,384,575\n",
      "Trainable params: 5,754,475\n",
      "Non-trainable params: 2,630,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(neural_image_captioning.model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../weights/nic-weights.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=True)\n",
    "\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_image_captioning.model.load_weights('../weights/nic-weights.hdf5')\n",
    "\n",
    "neural_image_captioning.model.compile(Adam(1e-4), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "404/468 [========================>.....] - ETA: 12s - loss: 5.3244"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "steps = (len(train_image2descriptions) * 5) // batch_size\n",
    "\n",
    "train_data_generator = data_generator(\n",
    "    train_image2descriptions,\n",
    "    train_image2embedding,\n",
    "    word2idx,\n",
    "    batch_size,\n",
    "    max_description_length,\n",
    "    len(vocabulary),\n",
    ")\n",
    "\n",
    "neural_image_captioning.model.fit_generator(\n",
    "    generator=train_data_generator,\n",
    "    steps_per_epoch=steps,\n",
    "    epochs=num_epochs,\n",
    "    verbose=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = NICInference(neural_image_captioning, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(path_to_img):\n",
    "    img = Image.open(path_to_img)\n",
    "    img.load()\n",
    "    plt.imshow(np.asarray(img, dtype='int32'));\n",
    "\n",
    "def evaluate(img_id, beam_width=3):\n",
    "    print('Image ID: {}'.format(img_id))\n",
    "    print('='*50)\n",
    "    print()\n",
    "    img = load_image(img_id)\n",
    "    \n",
    "    greedy_hypothesis = inference.greedy_search(img)\n",
    "    beam_hypothesis = inference.beam_search(img, beam_width=beam_width)\n",
    "    references = image_descriptions[img_id]\n",
    "\n",
    "    print('Greedy Search: {}'.format(greedy_hypothesis))\n",
    "    print('Beam Search: {}'.format(beam_hypothesis))\n",
    "    print()\n",
    "    \n",
    "    print('BLEU score Greedy Search: {}'.format(bleu_score.sentence_bleu(references, greedy_hypothesis)))\n",
    "    print('BLEU score Beam Search: {}'.format(bleu_score.sentence_bleu(references, beam_hypothesis)))\n",
    "    print()\n",
    "    \n",
    "    print('Greedy Search Hypothesis Log Probabilty: {}'.format(inference.predict_logprob(img, greedy_hypothesis)))\n",
    "    print('Beam Search Hypothesis Log Probabilty: {}'.format(inference.predict_logprob(img, beam_hypothesis)))\n",
    "    \n",
    "    show_img(IMAGES_DIR+img_id+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluate(\n",
    "    np.random.choice(list(train_image_descriptions.keys())), beam_width=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    np.random.choice(list(train_image_descriptions.keys())), beam_width=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    np.random.choice(list(train_image_descriptions.keys())), beam_width=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    np.random.choice(list(train_image_descriptions.keys())), beam_width=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    np.random.choice(list(train_image_descriptions.keys())), beam_width=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    np.random.choice(list(train_image_descriptions.keys())), beam_width=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
